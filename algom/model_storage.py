#!/usr/bin/env python

import pickle
import os
from datetime import datetime
from pandas import DataFrame
from re import search

from algom import configs
from algom.utils.storage_object import storageObject
from algom.utils.data_object import dataObject


GOOGLE_STORAGE_BUCKET = configs.GOOGLE_STORAGE_BUCKET
MODEL_STORAGE_DIRECTORY = configs.MODEL_STORAGE_DIRECTORY
MODEL_LOCAL_DIRECTORY = configs.MODEL_LOCAL_DIRECTORY


class modelStorage():
    """Load and get models locally or to/from Google Cloud Storage (GCS).

    To load a model to GCS, add the model_object generated by algoMosaic.
    To get a model from GCS, specify the model_id generated by algoMosaic.
    """
    def __init__(
        self,
        model_object=None,
        model_id=None,
        bucket_name=GOOGLE_STORAGE_BUCKET,
        model_storage_directory=MODEL_STORAGE_DIRECTORY,
        model_local_directory=MODEL_LOCAL_DIRECTORY
    ):
        self.storage = storageObject()
        self.model_object = model_object
        self.model_id = model_id
        self.bucket_name = bucket_name
        self.model_storage_directory = model_storage_directory
        self.model_local_directory = model_local_directory

    def _get_model_filename(self):
        # get name parameters
        model_date = datetime.now().strftime('%Y%m%d')
        model_type = self.model_object.metadata.model_type
        model_id = self.model_object.model_id
        # add if a single ticker; otherwise, add 'multi'
        name = '{}_{}_{}.pickle'.format(model_date, model_type, model_id)
        return name

    def _get_model_local_filepath(self):
        filename = self._get_model_filename()
        local_dir = MODEL_LOCAL_DIRECTORY
        return '{}{}'.format(local_dir, filename)

    def _get_model_storage_filepath(self):
        filename = self._get_model_filename()
        storage_dir = MODEL_STORAGE_DIRECTORY
        return '{}{}'.format(storage_dir, filename)

    def _set_model_storage_filepath_from_id(self, model_id):
        """Set model storage path and receive all Storage IDs.
            - model_id
            - model_storage_filepath
            - model_filename
        """
        blob_list = self.storage.get_blob_list(configs.GOOGLE_STORAGE_BUCKET)
        b = [f for f in blob_list if model_id in f]
        self.model_storage_filepath = b[0] if len(b) > 0 else None
        self.model_id = model_id or self.model_id
        if not self.model_storage_filepath:
            print("ERROR: Model cannot be found in Storage."
                  "Check your bucket and try again.")
        else:
            self.model_filename = search(
                r".*\/(.*)$", self.model_storage_filepath).group(1)
            print("SUCCESS: Model {} has been loaded successfully.".format(
                self.model_id))

    def load_model_to_file(self, filepath=None):
        """Output pickle of the model you created. Extracts parameters from
        Scikit-learn model, which is stored in `self` class.

        Parameters
        ----------
        model_filename: string; Name of the model (optional). Added
                    to the output name of the file.

        model_local_directory: string; Path where the pickle is stored locally.
                    Defaults to current working directory.
        """
        model_local_filepath = filepath or self.model_local_filepath
        if os.path.exists(model_local_filepath):
            os.remove(model_local_filepath)
        with open(model_local_filepath, "wb+") as filename:
            pickle.dump(self.model_object.model, filename)
        print('Dumped model to:\n\t{}'.format(self.model_local_filepath))

    def load_model_to_storage(
        self,
        local_filepath=None,
        storage_filepath=None,
        bucket_name=None
    ):
        """Load model to Google Cloud Storage as .pickle file."""
        self.bucket = bucket_name or self.bucket_name
        self.model_local_filepath = local_filepath or \
            self._get_model_local_filepath()
        self.model_storage_filepath = storage_filepath or \
            self._get_model_storage_filepath()

        # Write pickle to local file
        self.load_model_to_file(self.model_local_filepath)

        # Load model to GCS
        path = self.storage.upload_file(
            bucket_name=self.bucket,
            storage_path=self.model_storage_filepath,
            source_file_name=self.model_local_filepath
        )
        print("Uploaded pickle to Google Storage:\n\t{}".format(path))

        # Generate storage metadata and load to database
        self._get_storage_data()
        self.data.to_db(self.model_object.storage_table, if_exists='append')
        print("Uploaded storage metadata to Google BigQuery:\n\t{}".format(
            self.model_object.storage_table))

    def get_model_from_file(self, file_path):
        """Load model from a locally stored file."""
        model = pickle.load(open(file_path, "rb"))
        return model

    def get_model_from_storage(
        self,
        model_id=None,
        location='storage'
    ):
        # Initialize storage paths based on inputs
        self.model_id = model_id or self.model_id
        self._set_model_storage_filepath_from_id(self.model_id)

        # Get model from local file
        if location == 'local':
            self.model = self.get_model_from_file(
                self.model_local_directory + self.model_filename)

        # Get model from Google Storage
        if location == 'storage':
            self.storage.download_file(
                bucket_name=self.bucket_name,
                storage_path=self.model_storage_filepath,
                local_path=self.model_local_directory,
                destination_filename=self.model_filename
            )
            self.model = self.get_model_from_file(
                self.model_local_directory + self.model_filename
            )
        return self.model

    def _get_storage_data(self):
        """Add information related to storage instance to BigQuery.
        + model_id
        + model_storage_datetime
        + model_pickle_file
        + model_bucket_name
        """
        # specify data to include
        data = {
            'model_id': self.model_object.model_id,
            'model_storage_datetime': datetime.now(),
            'model_storage_date': datetime.now().strftime("%Y-%m-%d"),
            'model_storage_directory': self.model_storage_directory,
            'model_storage_filepath': self.model_storage_filepath,
            'model_bucket_name': self.bucket_name,
        }

        # convert to dataframe
        df_storage = DataFrame.from_dict(
            data=data,
            orient='index',
            columns=['value'])
        df_storage = df_storage.reset_index()
        df_storage['model_id'] = self.model_id
        df_storage['model_storage_datetime'] = datetime.now()
        df_storage.columns = [
            'parameter', 'value', 'model_id', 'model_storage_datetime']
        df_storage = df_storage[[
            'model_id', 'model_storage_datetime', 'parameter', 'value']]
        self.data = dataObject(df_storage)
